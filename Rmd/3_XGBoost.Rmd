---
title: "Home Credit Dafault Risk (XGBoost)"
author: "kotsubo takuto"
output: 
    html_document:
      md_extensions: -ascii_identifiers
      toc: true
      toc_depth: 3
---

# Home Credit Dafault Risk

## To do list

- XGBoostについて
    - xgboostの各種パラメータおよび, cvにおける意義を考える
    - 並列処理できるのか

## 参考サイト

- [xgboostに関する参考サイト](http://rtokei.tech/machine-learning/機械学習アルゴリズム〜xgboost〜/)
- [xgboostにおけるクロスバリデーション](http://puyokw.hatenablog.com/entry/2015/04/29/000557)
- [xgboostの欠損値処理について](https://medium.com/rv-data/missing-data-xgboost-and-r-part-2-9e47924d935a)

# Setting{.tabset .tabset-fade .tabset-pills}

## knitr option

```{r reset, include=FALSE}
# 初期化
rm(list = ls())
```

```{r set up, message=FALSE}
# set directory
setwd("~/Desktop/Home_Credit_Kaggle/") # for mac
#setwd("C:/Users/shiohama/Desktop/Home_Credit_Kaggle/") # for windows

# max.print 
options(max.print="100", digits=5)
# Global options
library(knitr)
opts_chunk$set(echo=TRUE,
               cache = FALSE,
	             prompt=FALSE,
               tidy=TRUE,
               comment=NA,
               message=FALSE,
               warning=FALSE)
opts_knit$set(width=75)
```

## Library package

- 必要なパッケージを適宜追加

```{r package, message=FALSE}
library(tidyverse)
library(readr) # for csv
library(xgboost)
library(caret)
library(doParallel)
library(DT)
```

## read csv

```{r}
# 訓練データ, テストデータ
# for mac
all_train <- read_csv("~/Desktop/Home_Credit_Kaggle/csv_imp/all_data_train.csv", na = c("XNA","NA","","NaN","?","Inf","-Inf")) %>% 
    mutate_if(is.character, funs(factor(.)))
all_test <- read_csv("~/Desktop/Home_Credit_Kaggle/csv_imp/all_data_test.csv", na = c("XNA","NA","","NaN","?","Inf","-Inf")) %>% 
    mutate_if(is.character, funs(factor(.)))
```

```{r}
# for windows
all_train <- read_csv("C:/Users/shiohama/Desktop/Home_Credit_Kaggle/csv_imp/all_data_train.csv", na = c("XNA","NA","","NaN","?","Inf","-Inf")) %>%
    mutate_if(is.character, funs(factor(.)))
all_test <- read_csv("C:/Users/shiohama/Desktop/Home_Credit_Kaggle/csv_imp/all_data_test.csv", na = c("XNA","NA","","NaN","?","Inf","-Inf")) %>%
    mutate_if(is.character, funs(factor(.)))
```

# XGBoost

- xgboost packageを用いて, クロスバリデーションを行う
- パラメータチューニングは最終的に行う

## Preprocessing 

- `application_train`のデータを利用して, xgboostを実装
- SK_ID_CURR のカラムを取り除く

```{r normal data}
# train data
train_data <- all_train %>% select(-SK_ID_CURR)
# test data
test_data <- all_test
```

## xgboost parameters

### best parameter 

- by kuroki

```{r}
# xgboost fitting with arbitrary parameters
xgb_params <- list(objective = "binary:logistic",
                   booster = "gbtree",
                   eval_metric = "auc",
                   nthread = 8,
                   eta = 0.05,
                   max_depth = 5,
                   min_child_weight = 30,
                   gamma = 0,
                   subsample = 0.85,
                   colsample_bytree = 0.65,
                   alpha = 0,
                   lambda = 0,
                   nrounds = 2000)
```

### other parameter

- by kaggle

```{r}
# xgboost fitting with arbitrary parameters
xgb_params = list(
  objective = "binary:logistic", # binary classification
  booster = "gbtree",
  eval_metric = "auc",
  nthread = 8,
  eta = 0.025,
  max_depth = 6,
  min_child_weight = 20,
  gamma = 0,
  subsample = 0.85,
  colsample_bytree = 0.7,
  colsample_bylevel = 0.632,
  alpha = 0,
  lambda = 0.05
)
```

## set file name 

```{r}
# set file name
file_name <- "best_para_Feature_990"
```

## xgboost excecute

- XgboostPred: 全てのパラメータを用いて, xgboostを実施する関数

- XgboostFeatureSelect: 選択した特徴量のみを用いて, xgboostを実施する関数

- XgboostFeatureSearch: すでに選択された特徴量をのぞいて, 優れた特徴量を探索する関数

```{r}
XgboostPred <- function(xgb_params,file_name,train_data,test_data){
  # set data
  train <- train_data %>% select(-TARGET) %>%  data.matrix()
  test <- test_data %>% select(-SK_ID_CURR) %>% data.matrix()
  target <- train_data$TARGET
  # set seed
  set.seed(831)
  # xgboost cross validation to choice best parameter
  xgb_cv <- 
    xgb.cv(data = xgb.DMatrix(data = train, label = target),
           params = xgb_params,
           missing = NA,
           nfold = 5, 
           nrounds = 2000,
           verbose = TRUE,
           prediction = TRUE,                                           # return the prediction using the final model 
           showsd = TRUE,                                               # standard deviation of loss across folds
           stratified = TRUE, 
           print_every_n = 10,
           early_stopping_rounds = 200 )
  # xgboost modeling  
  xgb_model <- 
    xgboost(data = xgb.DMatrix(data = train, label = target),
            params = xgb_params,
            nrounds = xgb_cv$best_iteration, # max number of trees to build
            verbose = TRUE,                                         
            print_every_n = 10,
            early_stopping_rounds = 200 )
  # Visualize important value
  xgb.importance(model = xgb_model) %>% 
    xgb.plot.importance(top_n = 30) %>% print()
  # predict
  pred_test <- predict(xgb_model,test)

  # make submit style
  submit <- data.frame(test_data,TARGET = pred_test) %>% 
    select(SK_ID_CURR,TARGET) %>% 
    mutate(SK_ID_CURR = SK_ID_CURR %>% as.integer())

  # file name + submit date
  path_name <- paste("~/Desktop/Home_Credit_Kaggle/submit/",file_name,Sys.Date(),".csv",sep = "") # for mac
  path_name <- paste("C:/Users/shiohama/Desktop/Home_Credit_Kaggle/submit/",file_name,Sys.Date(),".csv",sep = "") # for windows

  # export
  write_csv(submit,path = path_name)
  return(xgb_model)
}
```


```{r}
XgboostFeatureSelect <- function(xgb_params,file_name,train_data,test_data){
  # using column 
  alr <- read_tsv("~/Desktop/Home_Credit_Kaggle/data/best_para.tsv") # for mac
  alr <- read_tsv("C:/Users/shiohama/Desktop/Home_Credit_Kaggle/data/best_para.tsv") # for windows

  # set data
  train <- train_data %>% select(-TARGET) %>% select(alr$Feature) %>%  data.matrix()
  test <- test_data %>% select(-SK_ID_CURR) %>% select(alr$Feature) %>% data.matrix()
  target <- train_data$TARGET
  # set seed
  set.seed(831)
  # xgboost cross validation to choice best parameter
  xgb_cv <- 
    xgb.cv(data = xgb.DMatrix(data = train, label = target),
           params = xgb_params,
           missing = NA,
           nfold = 5, 
           nrounds = 2000,
           verbose = TRUE,
           prediction = TRUE,                                           # return the prediction using the final model 
           showsd = TRUE,                                               # standard deviation of loss across folds
           stratified = TRUE, 
           print_every_n = 10,
           early_stopping_rounds = 200 )
  # xgboost modeling  
  xgb_model <- 
    xgboost(data = xgb.DMatrix(data = train, label = target),
            params = xgb_params,
            nrounds = xgb_cv$best_iteration, # max number of trees to build
            verbose = TRUE,                                         
            print_every_n = 10,
            early_stopping_rounds = 200 )
  # Visualize important value
  xgb.importance(model = xgb_model) %>% 
    xgb.plot.importance(top_n = 30) %>% print()
  # predict
  pred_test <- predict(xgb_model,test)

  # make submit style
  submit <- data.frame(test_data,TARGET = pred_test) %>% 
    select(SK_ID_CURR,TARGET) %>% 
    mutate(SK_ID_CURR = SK_ID_CURR %>% as.integer())

  # file name + submit date
  path_name <- paste("~/Desktop/Home_Credit_Kaggle/submit/",file_name,Sys.Date(),".csv",sep = "") # for mac
  path_name <- paste("C:/Users/shiohama/Desktop/Home_Credit_Kaggle/submit/",file_name,Sys.Date(),".csv",sep = "") # for windows

  # export
  write_csv(submit,path = path_name)
  return(xgb_model)
}
```

```{r}
XgboostFeatureSearch <- function(xgb_params,train_data,test_data){
  # not need column
  alr <- read_tsv("~/Desktop/Home_Credit_Kaggle/data/best_para.tsv") # for mac
  alr <- read_tsv("C:/Users/shiohama/Desktop/Home_Credit_Kaggle/data/best_para.tsv") # for windows
  
  # set data
  train <- train_data %>% select(-TARGET) %>% select(-one_of(alr$Feature)) %>%  data.matrix()
  target <- train_data$TARGET 
  
  # set seed
  set.seed(831)
  # xgboost cross validation to choice best parameter
  xgb_cv <- 
    xgb.cv(data = xgb.DMatrix(data = train,label = target),
           params = xgb_params,
           missing = NA,
           nfold = 5, 
           nrounds = 2000,
           verbose = TRUE,
           prediction = TRUE,                                           # return the prediction using the final model 
           showsd = TRUE,                                               # standard deviation of loss across folds
           stratified = TRUE, 
           print_every_n = 10,
           early_stopping_rounds = 200 )
  # xgboost modeling  
  xgb_model <- 
    xgboost(data = xgb.DMatrix(data = train, label = target),
            params = xgb_params,
            nrounds = xgb_cv$best_iteration, # max number of trees to build
            verbose = TRUE,                                         
            print_every_n = 10,
            early_stopping_rounds = 200 )
  # extract Features
  tmp <- xgb.importance(model = xgb_model) %>% select(Feature)
  # 優れた50個を抽出し,元データと結合する
  tmp <- rbind(alr,tmp[1:50,])
  # save file
  write_tsv(tmp,"~/Desktop/Home_Credit_Kaggle/data/best_para.tsv") # for mac
  write_tsv(tmp,"C:/Users/shiohama/Desktop/Home_Credit_Kaggle/data/best_para.tsv") # for windows
}
```

## xgboost parameter tuning

- XgboostTune:
    - parameter setting
    - 5-fold cross-validation
    - output param and best iter
    - combine by rbind
    
```{r}
XgboostTune <- function(params, train,target){
  # function for as.list 
  one_entry <- function(x) {
    for (i in length(x)) attr(x[[i]], "names") <- NULL
    return(x)
  }
  xgb_params <- params %>% 
    mutate_if(is.factor, funs(as.character(.))) %>% 
    lapply(FUN=one_entry) 
  # set seed
  set.seed(831)
  # xgboost cross validation to choice best parameter
  xgb_cv <- 
    xgb.cv(data = xgb.DMatrix(data = train,label = target),
           params = xgb_params,
           missing = NA,
           nfold = 5, 
           nrounds = 2000,
           verbose = TRUE,
           prediction = TRUE,                                           # return the prediction using the final model 
           showsd = TRUE,                                               # standard deviation of loss across folds
           stratified = TRUE, 
           print_every_n = 10,
           early_stopping_rounds = 200 )
  best <- xgb_cv$best_iteration
  mat <- xgb_cv$evaluation_log
  xgb_params %>% 
    as.data.frame() %>% 
    cbind(mat[best,]) %>% 
    return()
}
```

```{r eval=FALSE}
# xgboost fitting with arbitrary parameters
xgbGrid <- expand.grid(
  objective = "binary:logistic", # binary classification
  booster = "gbtree",
  eval_metric = "auc",
  nthread = c(8),
  max_depth = c(5,6,7,8),
  eta = c(0.10,0.20,0.30),
  gamma = c(0),
  subsample = c(0.5,0.7),
  min_child_weight = c(5,10,20),
  colsample_bytree = c(0.3,0.5,0.7),
  colsample_bylevel = c(0.3,0.5,0.7),
  alpha = c(0),
  lambda = c(0.05))
```

```{r}
# preprocess
ans <- c()
# using column 
alr <- read_tsv("~/Desktop/Home_Credit_Kaggle/data/best_para.tsv") # for mac
alr <- read_tsv("C:/Users/shiohama/Desktop/Home_Credit_Kaggle/data/best_para.tsv") # for windows
# set data
train <- train_data %>% select(-TARGET) %>% select(alr$Feature) %>%  data.matrix()
target <- train_data$TARGET

# execute xgboost
for (i in 1:nrow(xgbGrid)) {
  sprintf("grid count: %d",i) %>% print() # 番号表示
  ans <- XgboostTune(params = xgbGrid[i,],train,target) %>% rbind(ans,.)
  write_tsv(ans,"~/Desktop/Home_Credit_Kaggle/data/tuning_result_feature_100.tsv") # for mac
  write_tsv(ans,"C:/Users/shiohama/Desktop/Home_Credit_Kaggle/data/tuning_result_feature_100.tsv") # for windows
}
```

## caret package

```{r eval=FALSE}
# hyper-parameters to be extracted with 10-fold cross validation repeated 2 times
trControl <- trainControl(method="cv",
                          number=5,
                          allowParallel = TRUE,
                          verboseIter = TRUE)

xgbGrid <- expand.grid(
  nrounds=c(350),
  max_depth = c(4, 6),
  eta = c(0.05, 0.1),
  gamma = c(0.01),
  colsample_bytree = c(0.75),
  subsample = c(0.50),
  min_child_weight = c(0))

# predict + parallel
cl <- makeCluster(detectCores())
registerDoParallel(cl)

## factor(TARGET) -> classifiacation
## TARGET -> regression
model_xgb <- train(TARGET ~ .,
                   data = data.matrix(train_data[1:100,]),
                   na.action = na.pass, # 欠損値に対する処理
                   trControl=trControl,
                   method='xgbTree',
                   objective = "binary:logistic",
                   tuneGrid = xgbGrid);

stopCluster(cl)
# result
print(model_xgb)

# predict
# 予測がマイナスの値を取ることがある.
# # 0=1のルールを考慮できていない??
# pred_caret <- predict(model_xgb, data.matrix(test_data %>% select(-SK_ID_CURR)))
```

