---
title: "Home Credit Dafault Risk (XGBoost)"
author: "kotsubo takuto"
output: 
    html_document:
      md_extensions: -ascii_identifiers
      toc: true
      toc_depth: 3
---

# Home Credit Dafault Risk

## To do list

- XGBoostについて
    - xgboostの各種パラメータおよび, cvにおける意義を考える
    - 並列処理できるのか
- EDAについて
    - 必要なパラメータの選択
    - まとめられるパラメータの選択
    - 新たな特徴量の作成

## 参考サイト

- [xgboostに関する参考サイト](http://rtokei.tech/machine-learning/機械学習アルゴリズム〜xgboost〜/)
- [xgboostにおけるクロスバリデーション](http://puyokw.hatenablog.com/entry/2015/04/29/000557)
- [xgboostの欠損値処理について](https://medium.com/rv-data/missing-data-xgboost-and-r-part-2-9e47924d935a)

# Setting{.tabset .tabset-fade .tabset-pills}

## knitr option

```{r reset, include=FALSE}
# 初期化
rm(list = ls())
```

```{r set up, message=FALSE}
# set directory
setwd("~/Desktop/Home_Credit_Kaggle/") 
# max.print 
options(max.print="100", digits=5)
# Global options
library(knitr)
opts_chunk$set(echo=TRUE,
               cache = TRUE,
	             prompt=FALSE,
               tidy=TRUE,
               comment=NA,
               message=FALSE,
               warning=FALSE)
opts_knit$set(width=75)
```

## Library package

- 必要なパッケージを適宜追加

```{r package, message=FALSE}
library(tidyverse)
library(readr) # for csv
library(xgboost)
library(caret)
library(doParallel)
library(DT)
```

## read csv

```{r}
# xgboost character 扱えない??
# mutate_if(is.character, funs(factor(.) %>% as.integer())) 

# 訓練データ, テストデータ
app_train <- read_csv("~/Desktop/Home_Credit_Kaggle/csv_imp/application_train_imp.csv",na = c("XNA","NA","","NaN","?")) %>% 
    mutate_if(is.character, funs(factor(.)))
app_test <- read_csv("~/Desktop/Home_Credit_Kaggle/csv_imp/application_test_imp.csv",na = c("XNA","NA","","NaN","?")) %>% 
  mutate_if(is.character, funs(factor(.)))
```

# XGBoost

- xgboost packageを用いて, クロスバリデーションを行う
- パラメータチューニングは最終的に行う

## Preprocessing 

- `application_train`のデータを利用して, xgboostを実装

```{r normal data}
# train data
train_data <- app_train %>% 
  select(-SK_ID_CURR)
# test data
test_data <- app_test
```

## xgboost package

### model

- 10分割交差検証法

```{r}
# xgboost fitting with arbitrary parameters
xgb_params = list(
  objective = "binary:logistic", # binary classification
  booster = "gbtree",
  eval_metric = "auc",
  nthread = 4,
  eta = 0.05,
  max_depth = 6,
  min_child_weight = 30,
  gamma = 0,
  subsample = 0.85,
  colsample_bytree = 0.7,
  colsample_bylevel = 0.632,
  alpha = 0,
  lambda = 0
)

# xgboost cross validation to choice best parameter
xgb_cv <- 
  xgb.cv(data = xgb.DMatrix(data = train_data %>% select(-TARGET) %>% data.matrix(),
                            label = train_data$TARGET),
         params = xgb_params,
         missing = NA,
         nfold = 10, 
         nrounds = 1000,
         verbose = TRUE,
         prediction = TRUE,                                           # return the prediction using the final model 
         showsd = TRUE,                                               # standard deviation of loss across folds
         stratified = TRUE, 
         print_every_n = 5,
         early_stopping_rounds = 50 )

#xgboost 
xgb_test <- 
  xgboost(data = xgb.DMatrix(data = train_data %>% select(-TARGET) %>% data.matrix(),
                             label = train_data$TARGET),
          params = xgb_params,
          nrounds = xgb_cv$best_iteration, # max number of trees to build
          verbose = TRUE,                                         
          print_every_n = 5,
          early_stopping_rounds = 50 )
```

### Visualize important value

```{r}
# 重要なパラメータの可視化
xgb.importance(model = xgb_test) %>% 
  xgb.plot.importance(top_n = 30)
```

### Predict

```{r}
# 予測関数
pred_test <- predict(xgb_test,data.matrix(test_data %>% select(-SK_ID_CURR)))
```

### make Submit file

```{r}
# submit file の構築
submit <- data.frame(test_data,TARGET = pred_test) %>% 
  select(SK_ID_CURR,TARGET)
# 結果の確認
# DT::datatable(submit,rownames = FALSE)
``` 

### export csv 

```{r}
# file name + submit date
file_name <- "impute-missing-value-xgb-"
path_name <- paste("submit/",file_name,Sys.Date(),sep = "")
# export
write_csv(submit,path = path_name)
```

## caret package

### model 

```{r,eval=FALSE}
# hyper-parameters to be extracted with 10-fold cross validation repeated 2 times
trControl <- trainControl(method="cv", 
                          number=5, 
                          allowParallel = TRUE,
                          verboseIter = TRUE)

xgbGrid <- expand.grid(
  nrounds=c(350),
  max_depth = c(4, 6),
  eta = c(0.05, 0.1),
  gamma = c(0.01),
  colsample_bytree = c(0.75),
  subsample = c(0.50),
  min_child_weight = c(0))

# predict + parallel
cl <- makeCluster(detectCores()) 
registerDoParallel(cl)

## factor(TARGET) -> classifiacation
## TARGET -> regression
model_xgb <- train(TARGET ~ ., 
                   data = data.matrix(train_data[1:100,]), 
                   na.action = na.pass, # 欠損値に対する処理
                   trControl=trControl, 
                   method='xgbTree', 
                   objective = "binary:logistic",
                   tuneGrid = xgbGrid);


stopCluster(cl)
# result
print(model_xgb)
```

### predict

- 予測がマイナスの値を取ることがある.
- 0=1のルールを考慮できていない??

```{r eval=FALSE}
pred_caret <- predict(model_xgb, data.matrix(test_data %>% select(-SK_ID_CURR)))
```

