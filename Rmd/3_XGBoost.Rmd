---
title: "Home Credit Dafault Risk (XGBoost)"
author: "kotsubo takuto"
output: 
    html_document:
      md_extensions: -ascii_identifiers
      toc: true
      toc_depth: 3
---

# Home Credit Dafault Risk

## To do list

- XGBoostについて
    - xgboostの各種パラメータおよび, cvにおける意義を考える
    - 並列処理できるのか

## 参考サイト

- [xgboostに関する参考サイト](http://rtokei.tech/machine-learning/機械学習アルゴリズム〜xgboost〜/)
- [xgboostにおけるクロスバリデーション](http://puyokw.hatenablog.com/entry/2015/04/29/000557)
- [xgboostの欠損値処理について](https://medium.com/rv-data/missing-data-xgboost-and-r-part-2-9e47924d935a)

# Setting{.tabset .tabset-fade .tabset-pills}

## knitr option

```{r reset, include=FALSE}
# 初期化
rm(list = ls())
```

```{r set up, message=FALSE}
# set directory
setwd("~/Desktop/Home_Credit_Kaggle/") 
# max.print 
options(max.print="100", digits=5)
# Global options
library(knitr)
opts_chunk$set(echo=TRUE,
               cache = FALSE,
	             prompt=FALSE,
               tidy=TRUE,
               comment=NA,
               message=FALSE,
               warning=FALSE)
opts_knit$set(width=75)
```

## Library package

- 必要なパッケージを適宜追加

```{r package, message=FALSE}
library(tidyverse)
library(readr) # for csv
library(xgboost)
library(caret)
library(doParallel)
library(DT)
```

## read csv

```{r}
# 訓練データ, テストデータ
app_train <- read_csv("~/Desktop/Home_Credit_Kaggle/csv_imp/all_data_train.csv", na = c("XNA","NA","","NaN","?","Inf","-Inf")) %>% 
    mutate_if(is.character, funs(factor(.)))
app_test <- read_csv("~/Desktop/Home_Credit_Kaggle/csv_imp/all_data_test.csv", na = c("XNA","NA","","NaN","?","Inf","-Inf")) %>% 
    mutate_if(is.character, funs(factor(.)))
```

# XGBoost

- xgboost packageを用いて, クロスバリデーションを行う
- パラメータチューニングは最終的に行う

## Preprocessing 

- `application_train`のデータを利用して, xgboostを実装

```{r normal data}
# train data
train_data <- app_train %>% select(-SK_ID_CURR)
# test data
test_data <- app_test
```

## xgboost function

- XgboostPred: 
    - parameter setting
    - 10-fold cross-validation
    - execute xgboost model
    - make submit file
- XgboostTune:
    - parameter setting
    - 10-fold cross-validation
    - output param and best iter
    - combine by rbind

```{r}
XgboostPred <- function(xgb_params,file_name,train_data,test_data){
  set.seed(831)
  # xgboost cross validation to choice best parameter
  xgb_cv <- 
    xgb.cv(data = xgb.DMatrix(data = train_data %>% select(-TARGET) %>% data.matrix(),
                              label = train_data$TARGET),
           params = xgb_params,
           missing = NA,
           nfold = 10, 
           nrounds = 2000,
           verbose = TRUE,
           prediction = TRUE,                                           # return the prediction using the final model 
           showsd = TRUE,                                               # standard deviation of loss across folds
           stratified = TRUE, 
           print_every_n = 10,
           early_stopping_rounds = 200 )
  # xgboost modeling  
  xgb_model <- 
    xgboost(data = xgb.DMatrix(data = train_data %>% select(-TARGET) %>% data.matrix(),
                               label = train_data$TARGET),
            params = xgb_params,
            nrounds = xgb_cv$best_iteration, # max number of trees to build
            verbose = TRUE,                                         
            print_every_n = 10,
            early_stopping_rounds = 200 )
  # Visualize important value
  xgb.importance(model = xgb_model) %>% 
    xgb.plot.importance(top_n = 30) %>% 
    print()
  # predict
  pred_test <- predict(xgb_model,data.matrix(test_data %>% select(-SK_ID_CURR)))

  # make submit style
  submit <- data.frame(test_data,TARGET = pred_test) %>% 
    select(SK_ID_CURR,TARGET)
  # file name + submit date
  path_name <- paste("~/Desktop/Home_Credit_Kaggle/submit/",file_name,Sys.Date(),sep = "")
  # export
  write_csv(submit,path = path_name)
}

XgboostTune <- function(params, train_data){
  # function for as.list 
  one_entry <- function(x) {
    for (i in length(x)) attr(x[[i]], "names") <- NULL
    return(x)
  }
  xgb_params <- params %>% 
    mutate_if(is.factor, funs(as.character(.))) %>% 
    lapply(FUN=one_entry) 
  # set seed
  set.seed(831)
  # xgboost cross validation to choice best parameter
  xgb_cv <- 
    xgb.cv(data = xgb.DMatrix(data = train_data %>% select(-TARGET) %>% data.matrix(),
                              label = train_data$TARGET),
           params = xgb_params,
           missing = NA,
           nfold = 10, 
           nrounds = 2000,
           verbose = TRUE,
           prediction = TRUE,                                           # return the prediction using the final model 
           showsd = TRUE,                                               # standard deviation of loss across folds
           stratified = TRUE, 
           print_every_n = 10,
           early_stopping_rounds = 200 )
  best <- xgb_cv$best_iteration
  mat <- xgb_cv$evaluation_log
  xgb_params %>% 
    as.data.frame() %>% 
    cbind(mat[best,]) %>% 
    return()
}
```

## execute xgboost model

- test 

```{r}
# xgboost fitting with arbitrary parameters
xgb_params = list(
  objective = "binary:logistic", # binary classification
  booster = "gbtree",
  eval_metric = "auc",
  nthread = 8,
  eta = 0.025,
  max_depth = 6,
  min_child_weight = 20,
  gamma = 0,
  subsample = 0.85,
  colsample_bytree = 0.7,
  colsample_bylevel = 0.632,
  alpha = 0,
  lambda = 0.05
)
# set file name
file_name <- "all_data"
# execute xgboost
XgboostPred(xgb_params,file_name,train_data,test_data)
```

## xgboost parameter tuning

```{r eval=FALSE}
# xgboost fitting with arbitrary parameters
xgbGrid <- expand.grid(
  objective = "binary:logistic", # binary classification
  booster = "gbtree",
  eval_metric = "auc",
  nthread = c(8),
  max_depth = c(6,7,8,9,10),
  eta = c(0.05, 0.1),
  gamma = c(0.01),
  subsample = c(0.85),
  min_child_weight = c(20),
  colsample_bytree = c(0.75),
  colsample_bylevel = c(0.632),
  alpha = c(0),
  lambda = c(0.05))

# execute xgboost
tmp <- XgboostTune(params = xgbGrid[1,],train_data[1:1000,])
```

## caret package

```{r eval=FALSE}
# hyper-parameters to be extracted with 10-fold cross validation repeated 2 times
trControl <- trainControl(method="cv",
                          number=5,
                          allowParallel = TRUE,
                          verboseIter = TRUE)

xgbGrid <- expand.grid(
  nrounds=c(350),
  max_depth = c(4, 6),
  eta = c(0.05, 0.1),
  gamma = c(0.01),
  colsample_bytree = c(0.75),
  subsample = c(0.50),
  min_child_weight = c(0))

# predict + parallel
cl <- makeCluster(detectCores())
registerDoParallel(cl)

## factor(TARGET) -> classifiacation
## TARGET -> regression
model_xgb <- train(TARGET ~ .,
                   data = data.matrix(train_data[1:100,]),
                   na.action = na.pass, # 欠損値に対する処理
                   trControl=trControl,
                   method='xgbTree',
                   objective = "binary:logistic",
                   tuneGrid = xgbGrid);

stopCluster(cl)
# result
print(model_xgb)

# predict
# 予測がマイナスの値を取ることがある.
# # 0=1のルールを考慮できていない??
# pred_caret <- predict(model_xgb, data.matrix(test_data %>% select(-SK_ID_CURR)))
```

