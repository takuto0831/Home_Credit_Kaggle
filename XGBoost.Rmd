---
title: "Home Credit Dafault Risk (XGBoost)"
author: "kotsubo takuto"
output: 
    html_document:
      md_extensions: -ascii_identifiers
      toc: true
      toc_depth: 3
---

# Home Credit Dafault Risk

## To do list

- 少数データにより xgboost model 構築
    - モデルを理解する
- XGBoostについて
    - xgboostの各種パラメータおよび, cvにおける意義を考える
    - 並列処理できるのか
- EDAについて
    - 必要なパラメータの選択
    - まとめられるパラメータの選択
    - 新たな特徴量の作成

## 参考サイト

- [xgboostに関する参考サイト](http://rtokei.tech/machine-learning/機械学習アルゴリズム〜xgboost〜/)
- [xgboostにおけるクロスバリデーション](http://puyokw.hatenablog.com/entry/2015/04/29/000557)
- [xgboostの欠損値処理について](https://medium.com/rv-data/missing-data-xgboost-and-r-part-2-9e47924d935a)

# Setting

## knitr option

```{r reset, include=FALSE}
# 初期化
rm(list = ls())
```

```{r set up, message=FALSE}
# set directory
setwd("~/Desktop/Home_Credit_Kaggle/") 
# max.print 
options(max.print="100", digits=5)
# Global options
library(knitr)
opts_chunk$set(echo=TRUE,
               cache = TRUE,
	             prompt=FALSE,
               tidy=TRUE,
               comment=NA,
               message=FALSE,
               warning=FALSE)
opts_knit$set(width=75)
```

## Library package

- 必要なパッケージを適宜追加

```{r package, message=FALSE}
library(tidyverse)
library(readr) # for csv
library(xgboost)
library(caret)
library(doParallel)
library(DT)
```

## Load funciton


## read csv

```{r}
# xgboost character 扱えない??
# mutate_if(is.character, funs(factor(.) %>% as.integer())) 

# 訓練データ, テストデータ
application_test <- read_csv("csv/application_test.csv")
application_train <- read_csv("csv/application_train.csv")

# クレジット利用履歴 (by Credit Bureau)
bureau <- read_csv("csv/bureau.csv")
bureau_balance <- read_csv("csv/bureau_balance.csv")

# 過去の利用データ
previous_application <- read_csv("csv/previous_application.csv")
installments_payments <- read_csv("csv/installments_payments.csv")
POS_CASH_balance <- read_csv("csv/POS_CASH_balance.csv")
credit_card_balance <- read_csv("csv/credit_card_balance.csv")

# submit style file
# sample_submission <- read_csv("csv/sample_submission.csv")
```

# XGBoost

- caret package による実装
- xgboost package による実装

## Preprocessing 

- `application_train`のデータを利用して, xgboostを実装

```{r}
# train data
train_data <- application_train %>% 
  select(-SK_ID_CURR) %>% 
  mutate_if(is.character, funs(factor(.)))

# test data
test_data <- application_test %>% 
  # select(SK_ID_CURR,CODE_GENDER,FLAG_OWN_CAR,FLAG_OWN_REALTY,FLAG_MOBIL) %>% 
  mutate_if(is.character, funs(factor(.)))
```

## `xgbosst` pacakge

### model

- 10分割交差検証法

```{r}
# xgboost fitting with arbitrary parameters
xgb_params = list(
  objective = "binary:logistic",                                              # binary classification
  booster = "gbtree",
  eval_metric = "auc",
  nthread = 4,
  eta = 0.05,
  max_depth = 6,
  min_child_weight = 30,
  gamma = 0,
  subsample = 0.85,
  colsample_bytree = 0.7,
  colsample_bylevel = 0.632,
  alpha = 0,
  lambda = 0
)

# xgboost cross validation to choice best parameter
xgb_cv <- 
  xgb.cv(data = data.matrix(train_data %>% 
                             select(-TARGET)),
         label = train_data$TARGET,
         params = xgb_params,
         missing = NA,
         nfold = 10, 
         nrounds = 1000,
         verbose = TRUE,
         prediction = TRUE,                                           # return the prediction using the final model 
         showsd = TRUE,                                               # standard deviation of loss across folds
         stratified = TRUE, 
         print_every_n = 1,
         early_stopping_rounds = 10 )

#xgboost 
xgb_test <- 
  xgboost(data = data.matrix(train_data %>% 
                             select(-TARGET)),
          label = train_data$TARGET,
          params = xgb_params,
          nrounds = xgb_cv$niter+1, # max number of trees to build
          verbose = TRUE,                                         
          print_every_n = 1,
          early_stopping_rounds = 10 )
```

### predict

```{r}
# 重要なパラメータの可視化
xgb.importance(model = xgb_test) %>% 
  xgb.plot.importance(top_n = 30)

# 予測関数
pred_test <- predict(xgb_test,data.matrix(test_data %>% select(-SK_ID_CURR)))
```

### make Submit file

```{r}
# submit file の構築
submit <- data.frame(test_data,TARGET = pred_test) %>% 
  select(SK_ID_CURR,TARGET)
# 結果の確認
DT::datatable(submit,rownames = FALSE)
``` 

### export csv 

```{r}
# file name + submit date
file_name <- "test-submit-xgb"
path_name <- paste("submit/",file_name,Sys.Date(),sep = "")
# export
write_csv(submit,path = path_name)
```

## `caret` package

### model 

```{r}
# hyper-parameters to be extracted with 10-fold cross validation repeated 2 times
trControl <- trainControl(method="repeatedcv", 
                          number=10, 
                          repeats=5,
                          verboseIter = TRUE)

xgbGrid <- expand.grid(
  nrounds=c(350),
  max_depth = c(4, 6),
  eta = c(0.05, 0.1),
  gamma = c(0.01),
  colsample_bytree = c(0.75),
  subsample = c(0.50),
  min_child_weight = c(0))

# predict + parallel
cl <- makeCluster(detectCores()) 
registerDoParallel(cl)

model_xgb <- train(factor(TARGET) ~ ., 
                   data = data.matrix(train_data), 
                   na.action = na.pass, # 欠損値に対する処理
                   trControl=trControl, 
                   method='xgbTree', 
                   tuneGrid = xgbGrid);
stopCluster(cl)
# result
print(model_xgb)
```

### predict

```{r}
pred_caret <- predict(model_xgb, data.matrix(test_data %>% select(-SK_ID_CURR)))
```

## make Submit file

```{r}
# submit file の構築
submit_caret <- data.frame(test_data,TARGET = pred_caret) %>% 
  select(SK_ID_CURR,TARGET)
# 結果の確認
DT::datatable(submit_caret,rownames = FALSE)
``` 

### export csv 

```{r}
# file name + submit date
file_name <- "test-submit-caret"
path_name <- paste("submit/",file_name,Sys.Date(),sep = "")
# export
write_csv(submit_caret,path = path_name)
```

